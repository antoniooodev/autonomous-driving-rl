# PPO configuration
algorithm:
  name: "ppo"
  hidden_dims: [256, 256]
  learning_rate: 0.0003
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  n_steps: 2048
  n_epochs: 10
  batch_size: 64
